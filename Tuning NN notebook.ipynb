{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":5,"outputs":[{"output_type":"stream","text":"/kaggle/input/pimaindiansdiabetescsv/pima-indians-diabetes.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/pimaindiansdiabetescsv/pima-indians-diabetes.csv')","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"   6  148  72  35    0  33.6  0.627  50  1\n0  1   85  66  29    0  26.6  0.351  31  0\n1  8  183  64   0    0  23.3  0.672  32  1\n2  1   89  66  23   94  28.1  0.167  21  0\n3  0  137  40  35  168  43.1  2.288  33  1\n4  5  116  74   0    0  25.6  0.201  30  0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>6</th>\n      <th>148</th>\n      <th>72</th>\n      <th>35</th>\n      <th>0</th>\n      <th>33.6</th>\n      <th>0.627</th>\n      <th>50</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>85</td>\n      <td>66</td>\n      <td>29</td>\n      <td>0</td>\n      <td>26.6</td>\n      <td>0.351</td>\n      <td>31</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8</td>\n      <td>183</td>\n      <td>64</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23.3</td>\n      <td>0.672</td>\n      <td>32</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>89</td>\n      <td>66</td>\n      <td>23</td>\n      <td>94</td>\n      <td>28.1</td>\n      <td>0.167</td>\n      <td>21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>137</td>\n      <td>40</td>\n      <td>35</td>\n      <td>168</td>\n      <td>43.1</td>\n      <td>2.288</td>\n      <td>33</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>116</td>\n      <td>74</td>\n      <td>0</td>\n      <td>0</td>\n      <td>25.6</td>\n      <td>0.201</td>\n      <td>30</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use scikit-learn to grid search the batch size and epochs\nimport numpy\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\n# Function to create model, required for KerasClassifier\ndef create_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(12, input_dim=8, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n# fix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\n# load dataset\ndataset = numpy.loadtxt(\"/kaggle/input/pimaindiansdiabetescsv/pima-indians-diabetes.csv\", delimiter=\",\")\n# split into input (X) and output (Y) variables\nX = dataset[:,0:8]\nY = dataset[:,8]\n# create model\nmodel = KerasClassifier(build_fn=create_model, verbose=0)\n# define the grid search parameters\nbatch_size = [10, 20, 40, 60, 80, 100]\nepochs = [10, 50, 100]\nparam_grid = dict(batch_size=batch_size, epochs=epochs)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(X, Y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use scikit-learn to grid search the batch size and epochs\nimport numpy\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\n# Function to create model, required for KerasClassifier\ndef create_model(optimizer='adam'):\n    # create model\n    model = Sequential()\n    model.add(Dense(12, input_dim=8, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model\n# fix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\n# load dataset\ndataset = numpy.loadtxt(\"/kaggle/input/pimaindiansdiabetescsv/pima-indians-diabetes.csv\", delimiter=\",\")\n# split into input (X) and output (Y) variables\nX = dataset[:,0:8]\nY = dataset[:,8]\n# create model\nmodel = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n# define the grid search parameters\noptimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\nparam_grid = dict(optimizer=optimizer)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(X, Y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use scikit-learn to grid search the learning rate and momentum\nimport numpy\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.optimizers import SGD\n# Function to create model, required for KerasClassifier\ndef create_model(learn_rate=0.01, momentum=0):\n    # create model\n    model = Sequential()\n    model.add(Dense(12, input_dim=8, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    # Compile model\n    optimizer = SGD(lr=learn_rate, momentum=momentum)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model\n# fix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\n# load dataset\ndataset = numpy.loadtxt(\"/kaggle/input/pimaindiansdiabetescsv/pima-indians-diabetes.csv\", delimiter=\",\")\n# split into input (X) and output (Y) variables\nX = dataset[:,0:8]\nY = dataset[:,8]\n# create model\nmodel = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n# define the grid search parameters\nlearn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\nmomentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\nparam_grid = dict(learn_rate=learn_rate, momentum=momentum)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(X, Y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use scikit-learn to grid search the weight initialization\nimport numpy\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\n# Function to create model, required for KerasClassifier\ndef create_model(init_mode='uniform'):\n    # create model\n    model = Sequential()\n    model.add(Dense(12, input_dim=8, kernel_initializer=init_mode, activation='relu'))\n    model.add(Dense(1, kernel_initializer=init_mode, activation='sigmoid'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n# fix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\n# load dataset\ndataset = numpy.loadtxt(\"/kaggle/input/pimaindiansdiabetescsv/pima-indians-diabetes.csv\", delimiter=\",\")\n# split into input (X) and output (Y) variables\nX = dataset[:,0:8]\nY = dataset[:,8]\n# create model\nmodel = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n# define the grid search parameters\ninit_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\nparam_grid = dict(init_mode=init_mode)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(X, Y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use scikit-learn to grid search the activation function\nimport numpy\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\n# Function to create model, required for KerasClassifier\ndef create_model(activation='relu'):\n    # create model\n    model = Sequential()\n    model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation=activation))\n    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n# fix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\n# load dataset\ndataset = numpy.loadtxt(\"/kaggle/input/pimaindiansdiabetescsv/pima-indians-diabetes.csv\", delimiter=\",\")\n# split into input (X) and output (Y) variables\nX = dataset[:,0:8]\nY = dataset[:,8]\n# create model\nmodel = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n# define the grid search parameters\nactivation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\nparam_grid = dict(activation=activation)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(X, Y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use scikit-learn to grid search the dropout rate\nimport numpy\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.constraints import maxnorm\n# Function to create model, required for KerasClassifier\ndef create_model(dropout_rate=0.0, weight_constraint=0):\n    # create model\n    model = Sequential()\n    model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='linear', kernel_constraint=maxnorm(weight_constraint)))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n# fix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\n# load dataset\ndataset = numpy.loadtxt(\"/kaggle/input/pimaindiansdiabetescsv/pima-indians-diabetes.csv\", delimiter=\",\")\n# split into input (X) and output (Y) variables\nX = dataset[:,0:8]\nY = dataset[:,8]\n# create model\nmodel = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n# define the grid search parameters\nweight_constraint = [1, 2, 3, 4, 5]\ndropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\nparam_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(X, Y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use scikit-learn to grid search the number of neurons\nimport numpy\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.constraints import maxnorm\n# Function to create model, required for KerasClassifier\ndef create_model(neurons=1):\n    # create model\n    model = Sequential()\n    model.add(Dense(neurons, input_dim=8, kernel_initializer='uniform', activation='linear', kernel_constraint=maxnorm(4)))\n    model.add(Dropout(0.2))\n    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n# fix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\n# load dataset\ndataset = numpy.loadtxt(\"/kaggle/input/pimaindiansdiabetescsv/pima-indians-diabetes.csv\", delimiter=\",\")\n# split into input (X) and output (Y) variables\nX = dataset[:,0:8]\nY = dataset[:,8]\n# create model\nmodel = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n# define the grid search parameters\nneurons = [1, 5, 10, 15, 20, 25, 30]\nparam_grid = dict(neurons=neurons)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(X, Y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n# Tips for Hyperparameter Optimization\n\n    k-fold Cross Validation. You can see that the results from the examples in this post show some variance. A default cross-validation of 3 was used, but perhaps k=5 or k=10 would be more stable. Carefully choose your cross validation configuration to ensure your results are stable.\n    Review the Whole Grid. Do not just focus on the best result, review the whole grid of results and look for trends to support configuration decisions.\n    Parallelize. Use all your cores if you can, neural networks are slow to train and we often want to try a lot of different parameters. Consider spinning up a lot of AWS instances.\n    Use a Sample of Your Dataset. Because networks are slow to train, try training them on a smaller sample of your training dataset, just to get an idea of general directions of parameters rather than optimal configurations.\n    Start with Coarse Grids. Start with coarse-grained grids and zoom into finer grained grids once you can narrow the scope.\n    Do not Transfer Results. Results are generally problem specific. Try to avoid favorite configurations on each new problem that you see. It is unlikely that optimal results you discover on one problem will transfer to your next project. Instead look for broader trends like number of layers or relationships between parameters.\n    Reproducibility is a Problem. Although we set the seed for the random number generator in NumPy, the results are not 100% reproducible. There is more to reproducibility when grid searching wrapped Keras models than is presented in this post.\n\n'''","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}